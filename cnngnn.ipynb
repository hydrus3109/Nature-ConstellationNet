{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-29T14:50:49.845307Z",
     "iopub.status.busy": "2024-08-29T14:50:49.844993Z",
     "iopub.status.idle": "2024-08-29T14:51:44.419666Z",
     "shell.execute_reply": "2024-08-29T14:51:44.418476Z",
     "shell.execute_reply.started": "2024-08-29T14:50:49.845267Z"
    },
    "id": "EpJxvJoEJpeO",
    "outputId": "64cb53e4-0852-4b11-cb99-ac6665d61596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.66.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.11.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2023.6.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.3.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.5.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.15.10)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.41)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.39.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.11/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (69.0.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.1.1+cu121.html\n",
      "Collecting pyg_lib\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/pyg_lib-0.4.0%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_spline_conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (935 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.0/936.0 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.11.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (1.26.3)\n",
      "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
      "Successfully installed pyg_lib-0.4.0+pt21cu121 torch_cluster-1.6.3+pt21cu121 torch_scatter-2.1.2+pt21cu121 torch_sparse-0.6.18+pt21cu121 torch_spline_conv-1.2.2+pt21cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting umap-learn\n",
      "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.3.0)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.66.1)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
      "Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba, pynndescent, umap-learn\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0 pynndescent-0.5.13 umap-learn-0.5.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting hdbscan\n",
      "  Downloading hdbscan-0.8.38.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20->hdbscan) (3.2.0)\n",
      "Downloading hdbscan-0.8.38.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hdbscan\n",
      "Successfully installed hdbscan-0.8.38.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:51:41.380154: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-29 14:51:41.380243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-29 14:51:41.382031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-29 14:51:41.392060: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 14:51:42.888993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install wandb\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.1+cu121.html\n",
    "!pip install umap-learn\n",
    "!pip install hdbscan\n",
    "import wandb\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from numpy.linalg import norm\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "#GNN stuff\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import umap\n",
    "import hdbscan\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By0LPq0vJpeP"
   },
   "source": [
    "Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T14:51:44.422899Z",
     "iopub.status.busy": "2024-08-29T14:51:44.422119Z",
     "iopub.status.idle": "2024-08-29T14:51:44.432809Z",
     "shell.execute_reply": "2024-08-29T14:51:44.431533Z",
     "shell.execute_reply.started": "2024-08-29T14:51:44.422863Z"
    },
    "id": "DyLdLSevJpeQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "  true_positives, false_positives, _, _ = _compute_counts(y_true, y_pred)\n",
    "  return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "  true_positives, _, false_negatives, _ = _compute_counts(y_true, y_pred)\n",
    "  return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "  true_positives, false_positives, false_negatives, true_negatives = _compute_counts(\n",
    "      y_true, y_pred)\n",
    "  return (true_positives + true_negatives) / (\n",
    "      true_positives + false_positives + false_negatives + true_negatives)\n",
    "\n",
    "\n",
    "def _compute_counts(y_true, y_pred):  # TODO(tsitsulin): add docstring pylint: disable=missing-function-docstring\n",
    "  contingency = contingency_matrix(y_true, y_pred)\n",
    "  same_class_true = np.max(contingency, 1)\n",
    "  same_class_pred = np.max(contingency, 0)\n",
    "  diff_class_true = contingency.sum(axis=1) - same_class_true\n",
    "  diff_class_pred = contingency.sum(axis=0) - same_class_pred\n",
    "  total = contingency.sum()\n",
    "\n",
    "  true_positives = (same_class_true * (same_class_true - 1)).sum()\n",
    "  false_positives = (diff_class_true * same_class_true * 2).sum()\n",
    "  false_negatives = (diff_class_pred * same_class_pred * 2).sum()\n",
    "  true_negatives = total * (\n",
    "      total - 1) - true_positives - false_positives - false_negatives\n",
    "\n",
    "  return true_positives, false_positives, false_negatives, true_negatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MnkJCmlJpeQ"
   },
   "source": [
    "Load High dimen MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T14:53:24.488176Z",
     "iopub.status.busy": "2024-08-29T14:53:24.487768Z",
     "iopub.status.idle": "2024-08-29T14:53:25.111028Z",
     "shell.execute_reply": "2024-08-29T14:53:25.110061Z",
     "shell.execute_reply.started": "2024-08-29T14:53:24.488149Z"
    },
    "id": "0cuUJQgYJpeQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70000, 784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataset = np.load('/notebooks/mnist.npy')\n",
    "#dataset = np.load('/notebooks/70ktransformeddataset.npy')\n",
    "file_path = '/notebooks/mnist.gt'\n",
    "#dataset = np.load('/notebooks/cifar10.npy')\n",
    "#dataset = np.load('/notebooks/70ktransformeddataset.npy')\n",
    "#dataset = np.load('HighDimenData/birds.npy')\n",
    "#file_path = 'HighDimenData/birds.gt'\n",
    "#dataset = np.load('HighDimenData/imagenet.npy')\n",
    "#file_path = 'HighDimenData/imagenet.gt'\n",
    "labels = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        labels.append(int(line.strip()))\n",
    "#labels = np.load('/notebooks/cifar10labels.npy')\n",
    "labels= torch.tensor(labels)\n",
    "dataset = torch.from_numpy(dataset)\n",
    "print(dataset.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raB6PgAHJpeQ"
   },
   "source": [
    "Neighbor graph gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T14:53:36.378459Z",
     "iopub.status.busy": "2024-08-29T14:53:36.377555Z",
     "iopub.status.idle": "2024-08-29T14:54:41.480943Z",
     "shell.execute_reply": "2024-08-29T14:54:41.480217Z",
     "shell.execute_reply.started": "2024-08-29T14:53:36.378417Z"
    },
    "id": "GZCLKg9FJpeR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "#5\n",
    "dataset_flat = dataset.reshape(dataset.shape[0], -1)\n",
    "k = 50\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(dataset_flat)\n",
    "distances, indices = nbrs.kneighbors(dataset_flat)\n",
    "\n",
    "# Calculate inverse distances, with a small epsilon to avoid division by zero\n",
    "epsilon = 1e-5\n",
    "inverse_distances = 1 / (distances.flatten() + epsilon)\n",
    "\n",
    "num_nodes = indices.shape[0]\n",
    "source_nodes = np.repeat(np.arange(num_nodes), k)\n",
    "target_nodes = indices.flatten()\n",
    "edge_index = np.vstack([source_nodes, target_nodes])\n",
    "#np.save('edgeindex70k.npy',edge_index)\n",
    "edge_index = torch.from_numpy(edge_index).long()\n",
    "edge_weight = torch.from_numpy(inverse_distances).float()\n",
    "#np.save('edgeweight70k.npy',edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T14:54:41.482585Z",
     "iopub.status.busy": "2024-08-29T14:54:41.482102Z",
     "iopub.status.idle": "2024-08-29T14:54:41.487227Z",
     "shell.execute_reply": "2024-08-29T14:54:41.486209Z",
     "shell.execute_reply.started": "2024-08-29T14:54:41.482549Z"
    },
    "id": "vUD0-Vx-JpeR"
   },
   "outputs": [],
   "source": [
    "train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "train_mask[:70000] = True\n",
    " \n",
    "# Creating the test mask\n",
    "test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "test_mask[20000:] = True\n",
    "\n",
    "train_mask = torch.from_numpy(train_mask)\n",
    "test_mask = torch.from_numpy(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDgoORQXJpeR"
   },
   "source": [
    "Planar Graph Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T14:54:41.489206Z",
     "iopub.status.busy": "2024-08-29T14:54:41.488572Z",
     "iopub.status.idle": "2024-08-29T14:54:41.968336Z",
     "shell.execute_reply": "2024-08-29T14:54:41.967103Z",
     "shell.execute_reply.started": "2024-08-29T14:54:41.489182Z"
    },
    "id": "Y5y66fgFJpeR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 1, 1,  ..., 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.loader import RandomNodeLoader\n",
    "from torch_geometric.data import Data\n",
    "data = Data(x=dataset, edge_index=edge_index, edge_weight = edge_weight, y = labels, train_mask = train_mask, test_mask = test_mask)\n",
    "#30\n",
    "train_loader = NeighborLoader(data, input_nodes = data.train_mask, num_neighbors=[30]*2, shuffle=True, num_workers = 2, batch_size =16)\n",
    "test_loader = NeighborLoader(data,input_nodes= data.test_mask, num_neighbors=[-1], shuffle=False, num_workers = 2,batch_size = 32)\n",
    "sampled_hetero_data = next(iter(train_loader))\n",
    "print(sampled_hetero_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:55:27.674460Z",
     "iopub.status.busy": "2024-08-28T20:55:27.673880Z",
     "iopub.status.idle": "2024-08-28T20:55:27.685893Z",
     "shell.execute_reply": "2024-08-28T20:55:27.685580Z",
     "shell.execute_reply.started": "2024-08-28T20:55:27.674436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fe48bab2990, raw_cell=\"\n",
      "\n",
      "class FireModule(nn.Module):\n",
      "    def __init__(se..\" store_history=True silent=False shell_futures=True cell_id=f891d4fb-6874-417b-aca5-cec3203fbc5e>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fe3e0b1fbd0, execution_count=17 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fe48bab2990, raw_cell=\"\n",
      "\n",
      "class FireModule(nn.Module):\n",
      "    def __init__(se..\" store_history=True silent=False shell_futures=True cell_id=f891d4fb-6874-417b-aca5-cec3203fbc5e> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
    "        super(FireModule, self).__init__()\n",
    "        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = F.relu(x)\n",
    "        return torch.cat([\n",
    "            self.expand1x1(x),\n",
    "            self.expand3x3(x)\n",
    "        ], 1)\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fire1 = FireModule(32, 8, 16, 16)\n",
    "        self.fire2 = FireModule(32, 8, 16, 16)\n",
    "        self.fire3 = FireModule(32, 16, 32, 32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_final = nn.Conv2d(64, 128, kernel_size=1)  # 128 output channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)  # Reshape to 28x28 images with 1 channel\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(self.fire1(x))\n",
    "        x = self.pool(self.fire2(x))\n",
    "        x = self.pool(self.fire3(x))\n",
    "        x = F.relu(self.conv_final(x))\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))  # Reduce to [batch_size, 128, 1, 1]\n",
    "        x = x.view(-1, 128)  # Flatten to [batch_size, 128]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:13:37.844444Z",
     "iopub.status.busy": "2024-08-28T20:13:37.844147Z",
     "iopub.status.idle": "2024-08-28T20:13:37.849358Z",
     "shell.execute_reply": "2024-08-28T20:13:37.848768Z",
     "shell.execute_reply.started": "2024-08-28T20:13:37.844426Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CIFAR10_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10_CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input: (3, 32, 32) -> Output: (64, 32, 32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Input: (64, 32, 32) -> Output: (128, 32, 32)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Input: (128, 16, 16) -> Output: (256, 16, 16)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces each dimension by half\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Adjusted based on the output from conv layers\n",
    "        self.fc2 = nn.Linear(512, 128)  # Output 128 features for GNN input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # After conv1: (64, 32, 32)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # After conv2 + pool: (128, 16, 16)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # After conv3 + pool: (256, 8, 8)\n",
    "        x = x.view(-1, 128 * 8 * 8)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:13:37.850513Z",
     "iopub.status.busy": "2024-08-28T20:13:37.850312Z",
     "iopub.status.idle": "2024-08-28T20:13:37.854768Z",
     "shell.execute_reply": "2024-08-28T20:13:37.854258Z",
     "shell.execute_reply.started": "2024-08-28T20:13:37.850494Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64* 7 * 7, 256)  # Flattening to 128 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28).to(device)  # Reshape to 28x28 images with 1 channel\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:55:42.356780Z",
     "iopub.status.busy": "2024-08-28T20:55:42.356304Z",
     "iopub.status.idle": "2024-08-28T20:55:42.368059Z",
     "shell.execute_reply": "2024-08-28T20:55:42.367705Z",
     "shell.execute_reply.started": "2024-08-28T20:55:42.356756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fe4b8577950, raw_cell=\"import torch\n",
      "from torch.nn import Sequential, Line..\" store_history=True silent=False shell_futures=True cell_id=b67c300d-f005-4551-9b09-28db50010607>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fe48b9627d0, execution_count=19 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fe4b8577950, raw_cell=\"import torch\n",
      "from torch.nn import Sequential, Line..\" store_history=True silent=False shell_futures=True cell_id=b67c300d-f005-4551-9b09-28db50010607> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, GCNConv\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import DenseGraphConv, GCNConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout_rate=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.pool = DMoNPooling([hidden_channels, hidden_channels], out_channels)\n",
    "        # Initial GAT layer\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, add_self_loops=True))\n",
    "        self.batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Hidden GAT layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels, add_self_loops=True))\n",
    "            self.batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Final GAT layer\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, add_self_loops=True))\n",
    "        self.batch_norms.append(BatchNorm1d(out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv, batch_norm in zip(self.convs[:-1], self.batch_norms[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "            #print(x.size())\n",
    "\n",
    "       # x = self.convs[-1](x, edge_index, edge_attr = edge_attr)\n",
    "        adj = to_dense_adj(edge_index)\n",
    "        #print(x.size())\n",
    "        x, _, adj, sp1, o1, c1 = self.pool(x, adj)\n",
    "        #print(x.size())\n",
    "\n",
    "        return F.log_softmax(x, dim=-1), sp1 + o1 + c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:55:34.844939Z",
     "iopub.status.busy": "2024-08-28T20:55:34.844408Z",
     "iopub.status.idle": "2024-08-28T20:55:34.857637Z",
     "shell.execute_reply": "2024-08-28T20:55:34.857323Z",
     "shell.execute_reply.started": "2024-08-28T20:55:34.844920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fe3b1cba410, raw_cell=\"import torch\n",
      "from torch.nn import Sequential, Line..\" store_history=True silent=False shell_futures=True cell_id=da5ea194-410a-4606-b9ba-57cd001adc77>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fe47ac51e90, execution_count=18 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fe3b1cba410, raw_cell=\"import torch\n",
      "from torch.nn import Sequential, Line..\" store_history=True silent=False shell_futures=True cell_id=da5ea194-410a-4606-b9ba-57cd001adc77> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SuperGATConv, global_mean_pool\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import DenseGraphConv, DMoNPooling, GCNConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "class SUPAGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, num_heads, dropout_rate=0.5):\n",
    "        super(SUPAGAT, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.pool = DMoNPooling([hidden_channels*num_heads, hidden_channels*num_heads], out_channels)\n",
    "        # Initial GAT layer\n",
    "        self.convs.append(SuperGATConv(in_channels, hidden_channels, heads = num_heads,attention_type='MX',edge_sample_ratio=0.8,is_undirected=True))\n",
    "        self.batch_norms.append(BatchNorm1d(hidden_channels*num_heads))\n",
    "\n",
    "        # Hidden GAT layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SuperGATConv(hidden_channels*num_heads, hidden_channels, heads = num_heads,attention_type='MX',edge_sample_ratio=0.8,  is_undirected=True))\n",
    "            self.batch_norms.append(BatchNorm1d(hidden_channels*num_heads))\n",
    "\n",
    "        # Final GAT layer\n",
    "        self.convs.append(SuperGATConv(hidden_channels*num_heads, out_channels, heads = num_heads,attention_type='MX',edge_sample_ratio=0.8,  is_undirected=True))\n",
    "        self.batch_norms.append(BatchNorm1d(out_channels*num_heads))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        att_loss = 0\n",
    "        for conv, batch_norm in zip(self.convs[:-1], self.batch_norms[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            att_loss += conv.get_attention_loss()\n",
    "            x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "       # x = self.convs[-1](x, edge_index, edge_attr = edge_attr)\n",
    "        adj = to_dense_adj(edge_index)\n",
    "       # print(x.size())\n",
    "        x, _, adj, sp1, o1, c1 = self.pool(x, adj)\n",
    "       # print(x.size())\n",
    "\n",
    "        return F.log_softmax(x, dim=-1), sp1 + o1 + c1+ att_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:55:46.145312Z",
     "iopub.status.busy": "2024-08-28T20:55:46.144600Z",
     "iopub.status.idle": "2024-08-28T20:55:46.153689Z",
     "shell.execute_reply": "2024-08-28T20:55:46.153326Z",
     "shell.execute_reply.started": "2024-08-28T20:55:46.145292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fe361712a10, raw_cell=\"class combine(torch.nn.Module):\n",
      "    def __init__(s..\" store_history=True silent=False shell_futures=True cell_id=4f4822b9-aba4-44ad-b7dc-582b01db24b4>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fe48b9bec90, execution_count=20 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fe361712a10, raw_cell=\"class combine(torch.nn.Module):\n",
      "    def __init__(s..\" store_history=True silent=False shell_futures=True cell_id=4f4822b9-aba4-44ad-b7dc-582b01db24b4> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "class combine(torch.nn.Module):\n",
    "    def __init__(self, cnn, gnn):\n",
    "        super(combine, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.gnn = gnn\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.cnn(x)  \n",
    "        #print(x.size())\n",
    "        x, loss = self.gnn(x, edge_index)  # Apply GNN\n",
    "\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:55:57.063009Z",
     "iopub.status.busy": "2024-08-28T20:55:57.062410Z",
     "iopub.status.idle": "2024-08-28T20:55:57.113811Z",
     "shell.execute_reply": "2024-08-28T20:55:57.113476Z",
     "shell.execute_reply.started": "2024-08-28T20:55:57.063005Z"
    },
    "id": "qD-mbRwcJpeR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fe3e0b2afd0, raw_cell=\"from torch.optim.lr_scheduler import CosineAnneali..\" store_history=True silent=False shell_futures=True cell_id=22d5e676-30b3-4a03-809a-23a1d3aec8e0>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fe48b9bce90, execution_count=21 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fe3e0b2afd0, raw_cell=\"from torch.optim.lr_scheduler import CosineAnneali..\" store_history=True silent=False shell_futures=True cell_id=22d5e676-30b3-4a03-809a-23a1d3aec8e0> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#gnn = SUPAGAT(in_channels= 128, hidden_channels=16, out_channels=10, num_heads =8, num_layers=5, dropout_rate=0.3).to(device)\n",
    "gnn = GCN(in_channels=  128, hidden_channels=64, out_channels=10, num_layers=5, dropout_rate=0.3).to(device)\n",
    "#cnn = CNN().to(device)\n",
    "cnn = SqueezeNet().to(device)\n",
    "#cnn = CIFAR10_CNN().to(device)\n",
    "\n",
    "model = combine(cnn, gnn)\n",
    "#model = GIN(in_channels= 128, hidden_channels=128, out_channels=10, num_layers=5, dropout_rate=0.3).to(device)\n",
    "#model = GCN(in_channels= 784, hidden_channels=625, out_channels=10, num_layers=5, dropout_rate=0.3).to(device)\n",
    "#model = GCNpool(in_channels= 784, hidden_channels=128, out_channels=10, num_layers=5, dropout_rate=0.3).to(device)\n",
    "#model = Net(in_channels= 784,out_channels=10,hidden_channels=128).to(device)\n",
    "#model = HGT(in_channels= 512, hidden_channels = 128, num_heads=2, out_channels=10, num_layers=5, dropout_rate=0.3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience= 5, factor = 0.75)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T20:13:43.525561Z",
     "iopub.status.busy": "2024-08-28T20:13:43.525217Z",
     "iopub.status.idle": "2024-08-28T20:13:43.530115Z",
     "shell.execute_reply": "2024-08-28T20:13:43.529552Z",
     "shell.execute_reply.started": "2024-08-28T20:13:43.525536Z"
    },
    "id": "LgHu8GuyJpeS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class lossmodel(nn.Module):\n",
    "    def __init__(self, n_clusters, dropout_rate = 0):\n",
    "        super(lossmodel, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.dropout_rate = 0\n",
    "\n",
    "        # Define the layers\n",
    "        self.transform = nn.Sequential(\n",
    "            nn.Linear(n_clusters, n_clusters),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.orthogonal_(self.transform[0].weight)\n",
    "        nn.init.zeros_(self.transform[0].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transform(x)\n",
    "\n",
    "\n",
    "def calculate_collapse_loss(number_of_nodes, n_clusters, output, lossize):\n",
    "    model = lossmodel(n_clusters).to(device)\n",
    "    assignments = F.softmax(model(output), dim=1)\n",
    "    cluster_sizes = torch.sum(assignments, dim=0)\n",
    "    collapse_loss = torch.norm(cluster_sizes) / number_of_nodes * torch.sqrt(torch.tensor(float(n_clusters))) - 1\n",
    "    return collapse_loss*lossize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.execute_input": "2024-08-28T20:56:26.248034Z",
     "iopub.status.busy": "2024-08-28T20:56:26.247544Z",
     "iopub.status.idle": "2024-08-28T20:56:33.510783Z",
     "shell.execute_reply": "2024-08-28T20:56:33.510400Z",
     "shell.execute_reply.started": "2024-08-28T20:56:26.248010Z"
    },
    "id": "mWk0RSVMKcvz",
    "outputId": "faac5327-3f5d-4145-b115-850a816596b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fe360787350>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fe3cd021250, raw_cell=\"config = {\n",
      "    \"architecture\":\"squeeze-net-gcn\",\n",
      " ..\" store_history=True silent=False shell_futures=True cell_id=8e6314b7-4bdc-4a11-9888-41161462b743>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f5s8tbvl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e2dae4b034062a6d17ef008531bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁█</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>nmi</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.73167</td></tr><tr><td>loss</td><td>2.71178</td></tr><tr><td>nmi</td><td>0.12911</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnngnn256</strong> at: <a href='https://wandb.ai/aidan_research/PRIMES-GNN/runs/f5s8tbvl' target=\"_blank\">https://wandb.ai/aidan_research/PRIMES-GNN/runs/f5s8tbvl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240828_201422-f5s8tbvl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f5s8tbvl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f185b0d2214b9f8219b9b133cbc0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113573877891112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240828_205626-x1uerd4f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aidan_research/PRIMES-GNN/runs/x1uerd4f' target=\"_blank\">squeezenetgcn</a></strong> to <a href='https://wandb.ai/aidan_research/PRIMES-GNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aidan_research/PRIMES-GNN' target=\"_blank\">https://wandb.ai/aidan_research/PRIMES-GNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aidan_research/PRIMES-GNN/runs/x1uerd4f' target=\"_blank\">https://wandb.ai/aidan_research/PRIMES-GNN/runs/x1uerd4f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"architecture\":\"squeeze-net-gcn\",\n",
    "    \"clusters\":10,\n",
    "    \"dataset\":\"mnist\",\n",
    "    \"epochs\":50,\n",
    "    \"hidden_channel\":16,\n",
    "    \"layers\":5,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"loss_function\":\"ALL\",\n",
    "    \"neighbors\":50,\n",
    "    \"batch-size\": 64,\n",
    "    \"umap-size\":256,\n",
    "    \"umap-neighbors\":50,\n",
    "    \"num-heads\": 8, \n",
    "    \"sampling-ratio\" : 0.8\n",
    "}\n",
    "\n",
    "# Pass the config dictionary when you initialize W&B\n",
    "run = wandb.init(project=\"PRIMES-GNN\", config=config, name = 'squeezenetgcn')\n",
    "#e807d0434ea6864dc92d6c99cdcfa073feaca0cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg06gp64JpeS"
   },
   "source": [
    "Neighbor loader with Plateau scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-08-28T21:36:56.145102Z",
     "iopub.status.busy": "2024-08-28T21:36:56.144663Z"
    },
    "id": "lJa03rVLJpeS",
    "outputId": "2c3a1274-354d-4abe-fb94-e13b2509f255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711.7996096611023\n",
      "711.8003051280975\n",
      "Epoch: 001, Train Loss: -0.289, Train Acc: 0.000,   Test Loss: 0.087, Test Acc: 0.921,NMI:0.652,ARI:0.588\n",
      "737.4336223602295\n",
      "737.4343955516815\n",
      "Epoch: 002, Train Loss: -0.366, Train Acc: 0.000,   Test Loss: 0.049, Test Acc: 0.926,NMI:0.693,ARI:0.629\n",
      "733.6329205036163\n",
      "733.6336350440979\n",
      "Epoch: 003, Train Loss: -0.404, Train Acc: 0.000,   Test Loss: 0.083, Test Acc: 0.925,NMI:0.707,ARI:0.634\n",
      "739.8320560455322\n",
      "739.8328478336334\n",
      "Epoch: 004, Train Loss: -0.422, Train Acc: 0.000,   Test Loss: 0.052, Test Acc: 0.933,NMI:0.732,ARI:0.670\n",
      "767.8897695541382\n",
      "767.8905045986176\n",
      "Epoch: 005, Train Loss: -0.430, Train Acc: 0.000,   Test Loss: 0.086, Test Acc: 0.925,NMI:0.708,ARI:0.629\n",
      "783.2146277427673\n",
      "783.2152409553528\n",
      "Epoch: 006, Train Loss: -0.435, Train Acc: 0.000,   Test Loss: 0.072, Test Acc: 0.929,NMI:0.713,ARI:0.649\n",
      "811.1472804546356\n",
      "811.1479165554047\n",
      "Epoch: 007, Train Loss: -0.440, Train Acc: 0.000,   Test Loss: 0.047, Test Acc: 0.933,NMI:0.734,ARI:0.680\n",
      "791.9827435016632\n",
      "791.9835188388824\n",
      "Epoch: 008, Train Loss: -0.443, Train Acc: 0.000,   Test Loss: 0.101, Test Acc: 0.928,NMI:0.715,ARI:0.643\n",
      "787.4464151859283\n",
      "787.4467990398407\n",
      "Epoch: 009, Train Loss: -0.445, Train Acc: 0.000,   Test Loss: 0.041, Test Acc: 0.943,NMI:0.770,ARI:0.724\n",
      "786.8528552055359\n",
      "786.8536541461945\n",
      "Epoch: 010, Train Loss: -0.448, Train Acc: 0.000,   Test Loss: 0.077, Test Acc: 0.930,NMI:0.724,ARI:0.655\n",
      "775.2669899463654\n",
      "775.2676475048065\n",
      "Epoch: 011, Train Loss: -0.451, Train Acc: 0.000,   Test Loss: 0.045, Test Acc: 0.940,NMI:0.763,ARI:0.708\n",
      "782.1350960731506\n",
      "782.1358618736267\n",
      "Epoch: 012, Train Loss: -0.451, Train Acc: 0.000,   Test Loss: 0.055, Test Acc: 0.937,NMI:0.747,ARI:0.689\n",
      "779.3474509716034\n",
      "779.3477623462677\n",
      "Epoch: 013, Train Loss: -0.450, Train Acc: 0.000,   Test Loss: 0.059, Test Acc: 0.939,NMI:0.757,ARI:0.704\n",
      "799.3848435878754\n",
      "799.385204076767\n",
      "Epoch: 014, Train Loss: -0.452, Train Acc: 0.000,   Test Loss: 0.046, Test Acc: 0.937,NMI:0.753,ARI:0.695\n",
      "769.500729560852\n",
      "769.5014238357544\n",
      "Epoch: 015, Train Loss: -0.456, Train Acc: 0.000,   Test Loss: 0.069, Test Acc: 0.933,NMI:0.730,ARI:0.669\n",
      "779.8963806629181\n",
      "779.8970537185669\n",
      "Epoch: 016, Train Loss: -0.458, Train Acc: 0.000,   Test Loss: 0.039, Test Acc: 0.937,NMI:0.749,ARI:0.696\n",
      "774.7883470058441\n",
      "774.7890334129333\n",
      "Epoch: 017, Train Loss: -0.460, Train Acc: 0.000,   Test Loss: 0.052, Test Acc: 0.937,NMI:0.751,ARI:0.695\n",
      "778.7850303649902\n",
      "778.785567522049\n",
      "Epoch: 018, Train Loss: -0.461, Train Acc: 0.000,   Test Loss: 0.040, Test Acc: 0.939,NMI:0.762,ARI:0.707\n"
     ]
    }
   ],
   "source": [
    "import  time  \n",
    "import torch_sparse\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score  \n",
    "classes = 10\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    count = 0\n",
    "    for data in train_loader:\n",
    "        count = count+1\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #out,tot_loss = model(data.x, data.edge_index, data.edge_weight)\n",
    "        out,tot_loss = model(data.x, data.edge_index)\n",
    "        out = out.squeeze()\n",
    "        loss = tot_loss + calculate_collapse_loss(data.num_nodes,classes,out,0.1)\n",
    "        #loss = F.cross_entropy(out, data.y) \n",
    "        loss_all += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_all/count\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_all = 0\n",
    "    nmi = 0\n",
    "    count = 0  \n",
    "    avgARI = 0\n",
    "    for data in loader:\n",
    "        count +=1\n",
    "        data.to(device)\n",
    "        #pred,tot_loss = model(data.x, data.edge_index, data.edge_weight)\n",
    "        pred,tot_loss = model(data.x, data.edge_index)\n",
    "        pred = pred.squeeze()\n",
    "        loss = tot_loss+ calculate_collapse_loss(data.num_nodes,classes,pred,0.1  )\n",
    "        #loss = F.cross_entropy(pred, data.y) \n",
    "        loss_all += loss.item()\n",
    "        correct += accuracy_score(data.y.cpu().detach().numpy(),pred.max(dim=-1)[1].cpu().detach().numpy())\n",
    "        nmi += normalized_mutual_info_score(data.y.cpu().detach().numpy(),pred.max(dim=-1)[1].cpu().detach().numpy())\n",
    "        avgARI +=adjusted_rand_score(data.y.cpu().detach().numpy(),pred.max(dim=-1)[1].cpu().detach().numpy())\n",
    "    return loss_all/count, correct/count, nmi/count, avgARI/count\n",
    "\n",
    "\n",
    "times = []\n",
    "best_nmi = 0\n",
    "for epoch in range(1, 70):\n",
    "    start = time.time()\n",
    "    train_loss = train()\n",
    "    print(time.time()-start)\n",
    "    #_, train_acc,nmi = test(train_loader)\n",
    "    train_acc = 0\n",
    "    print(time.time()-start)\n",
    "    test_loss, test_acc,nmi, ari = test(test_loader)\n",
    "    wandb.log({'acc': test_acc, 'loss': test_loss, \"nmi\": nmi, \"ari\":ari})\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, '\n",
    "          f'Train Acc: {train_acc:.3f},  '\n",
    "          f' Test Loss: {test_loss:.3f}, '\n",
    "          f'Test Acc: {test_acc:.3f},'\n",
    "          f'NMI:{nmi:.3f},'\n",
    "          f'ARI:{ari:.3f}')\n",
    "    times.append(time.time() - start)\n",
    "    scheduler.step(test_loss)\n",
    "    if nmi > best_nmi:\n",
    "        torch.save(model.state_dict(), 'gcnmodel_weights.pth')\n",
    "        best_nmi = nmi\n",
    "\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-28T20:50:46.790259Z",
     "iopub.status.idle": "2024-08-28T20:50:46.790495Z",
     "shell.execute_reply": "2024-08-28T20:50:46.790372Z",
     "shell.execute_reply.started": "2024-08-28T20:50:46.790362Z"
    }
   },
   "outputs": [],
   "source": [
    "import time   \n",
    "import torch_sparse\n",
    "from sklearn.metrics.cluster import nor malized_mutual_info_score\n",
    "classes = 10\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    count = 0\n",
    "    for data in train_loader:\n",
    "        count = count+1\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        x_dict = {\n",
    "            'node': data['node'].x\n",
    "        }\n",
    "        edge_index_dict = {\n",
    "            ('node', 'to', 'node'): data['node', 'to', 'node'].edge_index\n",
    "\n",
    "        }\n",
    "        out,tot_loss = model(x_dict,edge_index_dict)\n",
    "        out = out.squeeze()\n",
    "        #loss = tot_loss + calculate_collapse_loss(data.num_nodes,classes,out,0.1)\n",
    "        labels = data['node'].y\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        loss_all += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_all/count\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_all = 0\n",
    "    nmi = 0\n",
    "    count = 0\n",
    "    for data in loader:\n",
    "        count +=1\n",
    "        data.to(device)\n",
    "        x_dict = {\n",
    "            'node': data['node'].x\n",
    "        }\n",
    "        edge_index_dict = {\n",
    "            ('node', 'to', 'node'): data['node', 'to', 'node'].edge_index\n",
    "\n",
    "        }\n",
    "        pred,tot_loss = model(x_dict,edge_index_dict)\n",
    "        pred = pred.squeeze()\n",
    "        #loss = tot_loss+ calculate_collapse_loss(data.num_nodes,classes,pred,0.1)\n",
    "        labels = data['node'].y\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        labels = data['node'].y.cpu().detach().numpy()\n",
    "        loss_all += loss.item()\n",
    "        correct += accuracy_score(labels,pred.max(dim=-1)[1].cpu().detach().numpy())\n",
    "        nmi += normalized_mutual_info_score(labels,pred.max(dim=-1)[1].cpu().detach().numpy())\n",
    "    return loss_all/count, correct/count, nmi/count\n",
    "\n",
    "\n",
    "times = []\n",
    "best_nmi = 0\n",
    "for epoch in range(1, 70):\n",
    "    start = time.time()\n",
    "    train_loss = train()\n",
    "    print(time.time()-start)\n",
    "    _, train_acc,nmi = test(train_loader)\n",
    "    print(time.time()-start)\n",
    "    test_loss, test_acc,nmi = test(test_loader)\n",
    "    wandb.log({'acc': test_acc, 'loss': test_loss, \"nmi\": nmi})\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, '\n",
    "          f'Train Acc: {train_acc:.3f},  '\n",
    "          f' Test Loss: {test_loss:.3f}, '\n",
    "          f'Test Acc: {test_acc:.3f}')\n",
    "    times.append(time.time() - start)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-08-28T20:50:46.791183Z",
     "iopub.status.idle": "2024-08-28T20:50:46.791585Z",
     "shell.execute_reply": "2024-08-28T20:50:46.791511Z",
     "shell.execute_reply.started": "2024-08-28T20:50:46.791463Z"
    },
    "id": "SXkoQF-qJpeS",
    "outputId": "236460e7-1fe3-44bf-83b3-3ecae52b72a1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "def metrics():\n",
    "    model.eval()\n",
    "    avgacc = 0\n",
    "    avgARI = 0\n",
    "    loss_all = 0\n",
    "    avgprec = 0\n",
    "    avgrec = 0\n",
    "    count = 0\n",
    "    avgnmi = 0\n",
    "    for data in train_loader:\n",
    "        count = count +1\n",
    "        data.to(device)\n",
    "        pred, totloss = model(data.x, data.edge_index)\n",
    "        pred = pred.max(dim=-1)[1].cpu().detach().numpy()\n",
    "        pred = pred.squeeze()\n",
    "        labelsfin = data.y.cpu().detach().numpy()\n",
    "        avgacc += accuracy_score(labelsfin,pred)\n",
    "        avgrec += recall(labelsfin,pred)\n",
    "        avgprec +=precision(labelsfin,pred)\n",
    "        avgARI +=adjusted_rand_score(labelsfin,pred)\n",
    "        avgnmi += normalized_mutual_info_score(labelsfin,pred)\n",
    "    print(f\"acc score:{avgacc/count}\")\n",
    "    print(f\"recall score:{avgrec/count}\")\n",
    "    print(f\"precision score:{avgprec/count}\")\n",
    "    print(f\"F1:{2*avgprec*avgrec/(avgprec+avgrec)/count}\")\n",
    "    print(f\"ARI:{avgARI/count}\")\n",
    "    print(f\"NMI:{avgnmi/count}\")\n",
    "metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q97myTuJpeS"
   },
   "source": [
    "Metrics For eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-28T20:50:46.792025Z",
     "iopub.status.idle": "2024-08-28T20:50:46.792154Z",
     "shell.execute_reply": "2024-08-28T20:50:46.792085Z",
     "shell.execute_reply.started": "2024-08-28T20:50:46.792079Z"
    },
    "id": "FogE6h2ZJpeT",
    "outputId": "41e2b0cc-a273-46b7-8562-ede8cd1930a7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "def metrics():\n",
    "    model.eval()\n",
    "    avgacc = 0\n",
    "    avgARI = 0\n",
    "    loss_all = 0\n",
    "    avgprec = 0\n",
    "    avgrec = 0\n",
    "    count = 0\n",
    "    avgnmi = 0\n",
    "    for data in test_loader:\n",
    "        count = count +1\n",
    "        data.to(device)\n",
    "        pred = model(data.x, data.edge_index, data.edge_weight)\n",
    "        pred = pred.max(dim=-1)[1].cpu().detach().numpy()\n",
    "        pred = pred.squeeze()\n",
    "        labelsfin = data.y.cpu().detach().numpy()\n",
    "        avgacc += accuracy_score(labelsfin,pred)\n",
    "        avgrec += recall(labelsfin,pred)\n",
    "        avgprec +=precision(labelsfin,pred)\n",
    "        avgARI +=adjusted_rand_score(labelsfin,pred)\n",
    "        avgnmi += normalized_mutual_info_score(labelsfin,pred)\n",
    "    print(f\"acc score:{avgacc/count}\")\n",
    "    print(f\"recall score:{avgrec/count}\")\n",
    "    print(f\"precision score:{avgprec/count}\")\n",
    "    print(f\"F1:{2*avgprec*avgrec/(avgprec+avgrec)/count}\")\n",
    "    print(f\"ARI:{avgARI/count}\")\n",
    "    print(f\"NMI:{avgnmi/count}\")\n",
    "metrics()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
